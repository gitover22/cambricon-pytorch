/*************************************************************************
 * Copyright (C) [2019-2023] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/

#include "bang_internal.h"

namespace torch_mlu {
namespace ops {

#define SIZE_PER_REGION_L2NORM (MAX_NRAM_SIZE - 32 * 4)

__nram__ float temp_nram[32];
__nram__ char total_nram[SIZE_PER_REGION_L2NORM];

inline __mlu_func__ void ComputeInternal(float* tensor_nram,
                                         int num_align) {
  __bang_square(tensor_nram, tensor_nram, num_align);
  __bang_sumpool(temp_nram, tensor_nram,
                 /*channel*/32, /*height*/1, /*width*/num_align / 32,
                 /*kernel_height*/1, /*kernel_width*/num_align / 32,
                 /*stride_height*/1, /*stride_width*/1);
  __bang_reduce_sum(temp_nram, temp_nram, 32);
}

template <typename T>
inline __mlu_func__ void Compute(T* tensor_nram,
                                 int num_align) {
  ComputeInternal((float*)tensor_nram, num_align);
}

template <>
inline __mlu_func__ void Compute(half* tensor_nram,
                                 int num_align) {
  __bang_half2float((float*)tensor_nram, tensor_nram + SIZE_PER_REGION_L2NORM / 4, num_align);
  ComputeInternal((float*)tensor_nram, num_align);
}

template <typename T>
__mlu_func__ void ApplyL2norm(const AddressList& tensors, const SizeList& sizes,
                              float* output_buffer,
                              float* output_buffer_per_tensor, int tensor_num,
                              bool per_tensor, int32_t* overflow) {
  // Data
  int num_per_region = SIZE_PER_REGION_L2NORM / sizeof(float); // compute type is fixed as float
  T* tensor_nram = (T*)total_nram;
  int load_offset = sizeof(T) == 2 ? SIZE_PER_REGION_L2NORM / 4 : 0;

  int remains_chunck_num = 0; // assign each task average chuncks as possible
  // avoid int overflow for tensor_size near to 2 ^ 31(2G)
  int64_t tensor_size, chunck_num, last_chunck_size;
  int repeat_per_task, last_loop_chunck_num;
  int chunck_id; // chunck_id maybe minus
  int element_num;
  float per_core_sum = 0;
  float temp_partial_sum = 0.0;
  for (int tensor_id = 0; tensor_id < tensor_num; ++tensor_id) {
    tensor_size = sizes.sizes[tensor_id];

    chunck_num = ALIGN_UP_DIV(tensor_size, num_per_region);
    last_chunck_size = (tensor_size - 1) % num_per_region + 1;

    repeat_per_task = ALIGN_UP_DIV(chunck_num + remains_chunck_num, taskDim);
    last_loop_chunck_num = chunck_num % taskDim;

    T* tensor_gdram = (T*)tensors.addresses[tensor_id];
    for (int iter = 0; iter < repeat_per_task; ++iter) {
      chunck_id = iter * taskDim + taskId - remains_chunck_num;

      if (chunck_id > -1 && chunck_id < chunck_num) {
        element_num = chunck_id == chunck_num - 1 ? last_chunck_size : num_per_region;
        // bang_squre requires n * 128 bytes
        // __bang_half2float requires n * 64 elements
        // take 64 as the intersection
        int num_align = ALIGN_UP(element_num, 64);

        // Load
        if (num_align != element_num) {
          __bang_write_zero(tensor_nram + load_offset, num_align);
        }
        __memcpy(tensor_nram + load_offset,
                 tensor_gdram + chunck_id * num_per_region,
                 element_num * sizeof(T), GDRAM2NRAM);

        Compute(tensor_nram, num_align);

        temp_partial_sum = *temp_nram;
        per_core_sum += temp_partial_sum;
        if (per_tensor) {
          __bang_atomic_add(temp_nram + 1, output_buffer_per_tensor + tensor_id * taskDim + taskId, temp_partial_sum);
        }
      }
    }
    remains_chunck_num = (remains_chunck_num + last_loop_chunck_num) % taskDim;
  }
  __bang_atomic_add(temp_nram + 1, output_buffer + taskId, per_core_sum);
} 

__mlu_global__ void MLUMultiTensorL2Norm(AddressList tensors, SizeList sizes,
                                         float* output_buffer,
                                         float* output_buffer_per_tensor,
                                         int tensor_num, bool per_tensor,
                                         bool amp_opt, int32_t* overflow,
                                         cnrtDataType_t cnrt_type) {
  if (amp_opt && *overflow) {
    return;
  }
  switch (cnrt_type) {
    case CNRT_FLOAT32:
      ApplyL2norm<float>(tensors, sizes, output_buffer,
                   output_buffer_per_tensor, tensor_num, per_tensor,
                   overflow);
      break;
    case CNRT_FLOAT16:
      ApplyL2norm<half>(tensors, sizes, output_buffer, output_buffer_per_tensor,
                  tensor_num, per_tensor, overflow);
      break;
    default:
      break;
  }
}


__mlu_func__ void ReduceSumPerTensor(float* output_per_tensor,
                                     float* output_buffer_per_tensor_ptr,
                                     int tensor_num) {
  // divide tensor_num equally by cores except for the last core
  // NOTE [taskDim assumption]: the following codes make the assumption that
  // taskDims for cleanup kernel and ApplyL2norm kernel are the same,
  // please be careful to break that assumption.
  const int batch_num = ALIGN_UP_DIV(tensor_num, taskDim);
  for (int batch = 0; batch < batch_num; batch++) {
    int tensor_id = batch * taskDim + taskId;
    if (tensor_id < tensor_num) {
      float temp_sum = 0.0f;
      float* tensor_nram = (float*)total_nram;
      __memcpy(tensor_nram, output_buffer_per_tensor_ptr + tensor_id * taskDim, taskDim * sizeof(float), GDRAM2NRAM);
      for (int chunk_id = 0; chunk_id < taskDim; chunk_id++) {
        temp_sum += tensor_nram[chunk_id];
      }
      output_per_tensor[tensor_id] = sqrtf(temp_sum);
    }
  }
}


__mlu_global__ void MLUCleanUp(float* output, float* output_per_tensor,
                               float* output_buffer_ptr,
                               float* output_buffer_per_tensor_ptr,
                               bool per_tensor, int tensor_num, bool amp_opt,
                               int* overflow) {
  if (coreId == 0x80) {
    return;
  }

  if (amp_opt && *overflow) {
    return;
  }

  if (per_tensor) {
    ReduceSumPerTensor(output_per_tensor, output_buffer_per_tensor_ptr,
                   tensor_num);
  }

  if (taskId == taskDim - 1) {
    // see NOTE [taskDim assumption]
    float final_value = 0.0f;
    float* tensor_nram = (float*)total_nram;
    __memcpy(tensor_nram, output_buffer_ptr, taskDim * sizeof(float), GDRAM2NRAM);
    for (int i = 0; i < taskDim; i++) {
      final_value += tensor_nram[i];
    }
#if __BANG_ARCH__ >= 300
    if (isnan(final_value) || isinf(final_value)) {
      *overflow = 1;
    }
#endif
    *output = sqrtf(final_value);
  }
}


void bang_fused_l2_norm_internal(AddressList tensors, SizeList sizes,
                                 float* output_buffer_ptr,
                                 float* output_buffer_per_tensor_ptr,
                                 int tensor_num, bool per_tensor,
                                 int32_t* overflow, cnrtDim3_t k_dim,
                                 cnrtFunctionType_t k_type, cnrtQueue_t queue,
                                 cnrtDataType_t cnrt_type, bool amp_opt) {
  MLUMultiTensorL2Norm<<<k_dim, k_type, queue>>>(
      tensors, sizes, output_buffer_ptr, output_buffer_per_tensor_ptr,
      tensor_num, per_tensor, amp_opt, overflow, cnrt_type);
}

void bang_fused_l2_norm_clean_internal(
    float* output_ptr, float* output_per_tensor_ptr, float* output_buffer_ptr,
    float* output_buffer_per_tensor_ptr, bool per_tensor, int tensor_num,
    int* overflow, cnrtDim3_t k_dim, cnrtFunctionType_t k_type,
    cnrtQueue_t queue, bool amp_opt) {
  MLUCleanUp<<<k_dim, k_type, queue>>>(
      output_ptr, output_per_tensor_ptr, output_buffer_ptr,
      output_buffer_per_tensor_ptr, per_tensor, tensor_num, amp_opt, overflow);
}

} // namespace ops
} // namespace torch_mlu
