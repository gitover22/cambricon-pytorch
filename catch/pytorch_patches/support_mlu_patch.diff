diff --git a/aten/src/ATen/core/VariableFallbackKernel.cpp b/aten/src/ATen/core/VariableFallbackKernel.cpp
index ebc54d8e7cb..fa80916d230 100644
--- a/aten/src/ATen/core/VariableFallbackKernel.cpp
+++ b/aten/src/ATen/core/VariableFallbackKernel.cpp
@@ -52,6 +52,10 @@ TORCH_LIBRARY_IMPL(_, AutogradXLA, m) {
   m.fallback(torch::CppFunction::makeFallthrough());
 }
 
+TORCH_LIBRARY_IMPL(_, AutogradMLU, m) {
+  m.fallback(torch::CppFunction::makeFallthrough());
+}
+
 TORCH_LIBRARY_IMPL(_, AutogradLazy, m) {
   m.fallback(torch::CppFunction::makeFallthrough());
 }
diff --git a/aten/src/ATen/functorch/BatchedTensorImpl.h b/aten/src/ATen/functorch/BatchedTensorImpl.h
index 32098960457..c22c6e53728 100644
--- a/aten/src/ATen/functorch/BatchedTensorImpl.h
+++ b/aten/src/ATen/functorch/BatchedTensorImpl.h
@@ -147,6 +147,7 @@ constexpr DispatchKeySet kKeysToPropagateToWrapper({
   DispatchKey::Negative,
   DispatchKey::Conjugate,
   DispatchKey::XLA,
+  DispatchKey::MLU,
   DispatchKey::CUDA,
   DispatchKey::CPU,
 });
diff --git a/aten/src/ATen/functorch/TensorWrapper.cpp b/aten/src/ATen/functorch/TensorWrapper.cpp
index afd79943051..1e976c68127 100644
--- a/aten/src/ATen/functorch/TensorWrapper.cpp
+++ b/aten/src/ATen/functorch/TensorWrapper.cpp
@@ -60,7 +60,7 @@ void dumpTensorCout(const Tensor& tensor) {
 
 c10::intrusive_ptr<TensorWrapper> makeTensorWrapperPtr(const Tensor& tensor, int64_t level, bool should_be_alive) {
   auto keys_to_propagate = kKeysToPropagateToWrapper | DispatchKeySet({
-      DispatchKey::AutogradCPU, DispatchKey::AutogradCUDA, DispatchKey::AutogradXLA});
+      DispatchKey::AutogradCPU, DispatchKey::AutogradCUDA, DispatchKey::AutogradXLA, DispatchKey::AutogradMLU});
   auto key_set = getKeysToPropagateToWrapper(tensor, keys_to_propagate);
   key_set = key_set.add(DispatchKey::FuncTorchGradWrapper);
   if (should_be_alive) {
@@ -77,7 +77,7 @@ Tensor makeTensorWrapper(const Tensor& tensor, int64_t level, bool is_immutable)
   }
 
   auto keys_to_propagate = kKeysToPropagateToWrapper | DispatchKeySet({
-      DispatchKey::AutogradCPU, DispatchKey::AutogradCUDA, DispatchKey::AutogradXLA});
+      DispatchKey::AutogradCPU, DispatchKey::AutogradCUDA, DispatchKey::AutogradXLA, DispatchKey::AutogradMLU});
   auto key_set = getKeysToPropagateToWrapper(tensor, keys_to_propagate);
   key_set = key_set.add(DispatchKey::FuncTorchGradWrapper);
   auto life_handle = getLifeHandleForLevel(level);
diff --git a/c10/core/Backend.h b/c10/core/Backend.h
index 33113087913..a82d168f5fe 100644
--- a/c10/core/Backend.h
+++ b/c10/core/Backend.h
@@ -55,6 +55,7 @@ enum class Backend {
   HPU,
   Lazy,
   PrivateUse1,
+  MLU,
   NumOptions
 };
 
@@ -113,6 +114,8 @@ static inline Backend dispatchKeyToBackend(DispatchKey t) {
     return Backend::HPU;
   } else if (t == DispatchKey::PrivateUse1) {
     return Backend::PrivateUse1;
+  } else if (t == DispatchKey::MLU || t == DispatchKey::AutogradMLU) {
+    return Backend::MLU;
   } else if (t == DispatchKey::Undefined) {
     return Backend::Undefined;
   } else {
@@ -176,6 +179,8 @@ static inline DispatchKey backendToDispatchKey(Backend b) {
       return DispatchKey::HPU;
     case Backend::PrivateUse1:
       return DispatchKey::PrivateUse1;
+    case Backend::MLU:
+      return DispatchKey::MLU;
     default:
       throw std::runtime_error("Unknown backend");
   }
@@ -234,6 +239,8 @@ static inline DeviceType backendToDeviceType(Backend b) {
       return DeviceType::HPU;
     case Backend::PrivateUse1:
       return DeviceType::PrivateUse1;
+    case Backend::MLU:
+      return DeviceType::MLU;
     case Backend::Undefined:
       TORCH_CHECK(false, "Undefined backend is not a valid device type");
     default:
@@ -298,6 +305,8 @@ static inline const char* toString(Backend b) {
       return "HPU";
     case Backend::PrivateUse1:
       return "PrivateUseOne";
+    case Backend::MLU:
+      return "MLU";
     default:
       return "UNKNOWN_BACKEND";
   }
diff --git a/c10/core/Device.cpp b/c10/core/Device.cpp
index 7b55d2dbe28..4ccaa4e1a15 100644
--- a/c10/core/Device.cpp
+++ b/c10/core/Device.cpp
@@ -37,6 +37,7 @@ DeviceType parse_type(const std::string& device_string) {
           {"meta", DeviceType::Meta},
           {"hpu", DeviceType::HPU},
           {"privateuseone", DeviceType::PrivateUse1},
+          {"mlu", DeviceType::MLU},
       }};
   auto device = std::find_if(
       types.begin(),
diff --git a/c10/core/Device.h b/c10/core/Device.h
index d53ab38ff9c..f4af35ad477 100644
--- a/c10/core/Device.h
+++ b/c10/core/Device.h
@@ -116,6 +116,11 @@ struct C10_API Device final {
     return type_ == DeviceType::HPU;
   }
 
+  /// Return true if the device is of HPU type.
+  bool is_mlu() const noexcept {
+    return type_ == DeviceType::MLU;
+  }
+  
   /// Return true if the device is of Lazy type.
   bool is_lazy() const noexcept {
     return type_ == DeviceType::Lazy;
diff --git a/c10/core/DeviceType.cpp b/c10/core/DeviceType.cpp
index ac4c1f653ef..e00b0c7aa89 100644
--- a/c10/core/DeviceType.cpp
+++ b/c10/core/DeviceType.cpp
@@ -47,6 +47,8 @@ std::string DeviceTypeName(DeviceType d, bool lower_case) {
       return lower_case ? "ipu" : "IPU";
     case DeviceType::PrivateUse1:
       return lower_case ? "privateuseone" : "PRIVATEUSEONE";
+    case DeviceType::MLU:
+      return lower_case ? "mlu" : "MLU";
     default:
       TORCH_CHECK(
           false,
@@ -90,6 +92,7 @@ bool isValidDeviceType(DeviceType d) {
     case DeviceType::HPU:
     case DeviceType::IPU:
     case DeviceType::PrivateUse1:
+    case DeviceType::MLU:
       return true;
     default:
       return false;
diff --git a/c10/core/DeviceType.h b/c10/core/DeviceType.h
index 000ad331828..e221af22ec7 100644
--- a/c10/core/DeviceType.h
+++ b/c10/core/DeviceType.h
@@ -20,6 +20,7 @@ namespace c10 {
   _(CUDA, extra)                                  \
   _(HIP, extra)                                   \
   _(XLA, extra)                                   \
+  _(MLU, extra)                                   \
   _(MPS, extra)                                   \
   _(IPU, extra)                                   \
   _(XPU, extra)                                   \
@@ -50,11 +51,12 @@ enum class DeviceType : int8_t {
   Lazy = 17, // Lazy Tensors
   IPU = 18, // Graphcore IPU
   PrivateUse1 = 19, // PrivateUse1 device
+  MLU = 20, // MLU
   // NB: If you add more devices:
   //  - Change the implementations of DeviceTypeName and isValidDeviceType
   //    in DeviceType.cpp
   //  - Change the number below
-  COMPILE_TIME_MAX_DEVICE_TYPES = 20,
+  COMPILE_TIME_MAX_DEVICE_TYPES = 21,
 };
 
 constexpr DeviceType kCPU = DeviceType::CPU;
@@ -73,13 +75,14 @@ constexpr DeviceType kVE = DeviceType::VE;
 constexpr DeviceType kLazy = DeviceType::Lazy;
 constexpr DeviceType kIPU = DeviceType::IPU;
 constexpr DeviceType kPrivateUse1 = DeviceType::PrivateUse1;
+constexpr DeviceType kMLU = DeviceType::MLU;
 
 // define explicit int constant
 constexpr int COMPILE_TIME_MAX_DEVICE_TYPES =
     static_cast<int>(DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES);
 
 static_assert(
-    COMPILE_TIME_MAX_DEVICE_TYPES <= 20,
+    COMPILE_TIME_MAX_DEVICE_TYPES <= 32,
     "Hey!  You seem to be adding a lot of new DeviceTypes.  The intent was "
     "for this constant to reflect the actual number of DeviceTypes we support "
     "in PyTorch; it's important that this number is not too large as we "
diff --git a/c10/core/DispatchKey.cpp b/c10/core/DispatchKey.cpp
index 1da32dcc9b2..709b07b4030 100644
--- a/c10/core/DispatchKey.cpp
+++ b/c10/core/DispatchKey.cpp
@@ -15,6 +15,8 @@ const char* toString(BackendComponent t) {
       return "HIPBit";
     case BackendComponent::XLABit:
       return "XLABit";
+    case BackendComponent::MLUBit:
+      return "MLUBit";
     case BackendComponent::LazyBit:
       return "LazyBit";
     case BackendComponent::MetaBit:
@@ -293,6 +295,7 @@ c10::DispatchKey parseDispatchKey(const std::string& k) {
       {"CUDA", c10::DispatchKey::CUDA},
       {"HIP", c10::DispatchKey::HIP},
       {"XLA", c10::DispatchKey::XLA},
+      {"MLU", c10::DispatchKey::MLU},
       {"MPS", c10::DispatchKey::MPS},
       {"XPU", c10::DispatchKey::XPU},
       {"IPU", c10::DispatchKey::IPU},
@@ -317,6 +320,7 @@ c10::DispatchKey parseDispatchKey(const std::string& k) {
 
       {"AutogradCPU", c10::DispatchKey::AutogradCPU},
       {"AutogradCUDA", c10::DispatchKey::AutogradCUDA},
+      {"AutogradMLU", c10::DispatchKey::AutogradMLU},
       {"AutogradXLA", c10::DispatchKey::AutogradXLA},
       {"AutogradLazy", c10::DispatchKey::AutogradLazy},
       {"AutogradMeta", c10::DispatchKey::AutogradMeta},
diff --git a/c10/core/DispatchKey.h b/c10/core/DispatchKey.h
index 536a15a24a0..62864c90741 100644
--- a/c10/core/DispatchKey.h
+++ b/c10/core/DispatchKey.h
@@ -32,6 +32,7 @@ namespace c10 {
   _(CUDA, extra)                                \
   _(HIP, extra)                                 \
   _(XLA, extra)                                 \
+  _(MLU, extra)                                 \
   _(MPS, extra)                                 \
   _(IPU, extra)                                 \
   _(XPU, extra)                                 \
diff --git a/c10/core/DispatchKeySet.cpp b/c10/core/DispatchKeySet.cpp
index a8f60451be3..37aa1f33f82 100644
--- a/c10/core/DispatchKeySet.cpp
+++ b/c10/core/DispatchKeySet.cpp
@@ -116,6 +116,8 @@ DispatchKeySet getBackendKeySetFromAutograd(DispatchKey t) {
       return DispatchKeySet(DispatchKey::CUDA);
     case DispatchKey::AutogradXLA:
       return DispatchKeySet(DispatchKey::XLA);
+    case DispatchKey::AutogradMLU:
+      return DispatchKeySet(DispatchKey::MLU);
     case DispatchKey::AutogradLazy:
       return DispatchKeySet(DispatchKey::Lazy);
     case DispatchKey::AutogradMeta:
diff --git a/c10/core/DispatchKeySet.h b/c10/core/DispatchKeySet.h
index cf07bb31611..94ba2a282a6 100644
--- a/c10/core/DispatchKeySet.h
+++ b/c10/core/DispatchKeySet.h
@@ -642,6 +642,7 @@ constexpr DispatchKeySet autocast_dispatch_keyset = DispatchKeySet({
     DispatchKey::AutocastCPU,
     DispatchKey::AutocastCUDA,
     DispatchKey::AutocastXPU,
+    DispatchKey::AutocastMLU,
 });
 
 // See Note [TLS Initialization]
@@ -654,6 +655,7 @@ constexpr DispatchKeySet default_excluded_set = DispatchKeySet({
     DispatchKey::AutocastCPU,
     DispatchKey::AutocastCUDA,
     DispatchKey::AutocastXPU,
+    DispatchKey::AutocastMLU,
 });
 
 constexpr DispatchKeySet autograd_dispatch_keyset_with_ADInplaceOrView =
@@ -732,6 +734,7 @@ constexpr auto autograd_cpu_ks = DispatchKeySet(DispatchKey::AutogradCPU);
 constexpr auto autograd_ipu_ks = DispatchKeySet(DispatchKey::AutogradIPU);
 constexpr auto autograd_xpu_ks = DispatchKeySet(DispatchKey::AutogradXPU);
 constexpr auto autograd_cuda_ks = DispatchKeySet(DispatchKey::AutogradCUDA);
+constexpr auto autograd_mlu_ks = DispatchKeySet(DispatchKey::AutogradMLU);
 constexpr auto autograd_xla_ks = DispatchKeySet(DispatchKey::AutogradXLA);
 constexpr auto autograd_lazy_ks = DispatchKeySet(DispatchKey::AutogradLazy);
 constexpr auto autograd_meta_ks = DispatchKeySet(DispatchKey::AutogradMeta);
@@ -807,6 +810,8 @@ inline DispatchKeySet getAutogradRelatedKeySetFromBackend(BackendComponent t) {
       return inplace_or_view_ks | autograd_xpu_ks;
     case BackendComponent::CUDABit:
       return inplace_or_view_ks | autograd_cuda_ks;
+    case BackendComponent::MLUBit:
+      return inplace_or_view_ks | autograd_mlu_ks;
     case BackendComponent::XLABit:
       return inplace_or_view_ks | autograd_xla_ks;
     case BackendComponent::LazyBit:
@@ -833,6 +838,7 @@ inline DispatchKeySet getAutocastRelatedKeySetFromBackend(BackendComponent t) {
   constexpr auto autocast_cpu_ks = DispatchKeySet(DispatchKey::AutocastCPU);
   constexpr auto autocast_xpu_ks = DispatchKeySet(DispatchKey::AutocastXPU);
   constexpr auto autocast_cuda_ks = DispatchKeySet(DispatchKey::AutocastCUDA);
+  constexpr auto autocast_mlu_ks = DispatchKeySet(DispatchKey::AutocastMLU);
   switch (t) {
     case BackendComponent::CPUBit:
       return autocast_cpu_ks;
@@ -841,6 +847,8 @@ inline DispatchKeySet getAutocastRelatedKeySetFromBackend(BackendComponent t) {
     case BackendComponent::CUDABit:
     case BackendComponent::XLABit:
       return autocast_cuda_ks;
+    case BackendComponent::MLUBit:
+      return autocast_mlu_ks;
     default:
       return DispatchKeySet();
   }
diff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index 707ebeb19e8..b2c90aa09d4 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -466,6 +466,7 @@ void check_base_legacy_new(
         c10::DispatchKey::CUDA,
         c10::DispatchKey::HIP,
         c10::DispatchKey::XLA,
+        c10::DispatchKey::MLU,
         c10::DispatchKey::Lazy,
         c10::DispatchKey::IPU,
         c10::DispatchKey::XPU,
diff --git a/torch/csrc/utils/tensor_types.cpp b/torch/csrc/utils/tensor_types.cpp
index 8a20c93d87a..edaf5026438 100644
--- a/torch/csrc/utils/tensor_types.cpp
+++ b/torch/csrc/utils/tensor_types.cpp
@@ -29,6 +29,8 @@ static const char* backend_to_string(const at::Backend& backend) {
       return "torch.xpu";
     case at::Backend::IPU:
       return "torch.ipu";
+    case at::Backend::MLU:
+      return "torch.mlu";
     case at::Backend::SparseCPU:
       return "torch.sparse";
     case at::Backend::SparseCUDA:
diff --git a/torch/library.h b/torch/library.h
index 69175d07566..6d2f5237630 100644
--- a/torch/library.h
+++ b/torch/library.h
@@ -354,6 +354,8 @@ inline CppFunction dispatch(c10::DeviceType type, Func&& raw_f) {
         return c10::DispatchKey::IPU;
       case c10::DeviceType::XLA:
         return c10::DispatchKey::XLA;
+      case c10::DeviceType::MLU:
+        return c10::DispatchKey::MLU;
       case c10::DeviceType::Lazy:
         return c10::DispatchKey::Lazy;
       case c10::DeviceType::MPS:
