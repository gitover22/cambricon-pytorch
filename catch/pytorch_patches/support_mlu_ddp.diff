diff --git a/torch/csrc/distributed/c10d/reducer.cpp b/torch/csrc/distributed/c10d/reducer.cpp
index 1a644927ce..42829674d4 100644
--- a/torch/csrc/distributed/c10d/reducer.cpp
+++ b/torch/csrc/distributed/c10d/reducer.cpp
@@ -296,6 +296,11 @@ void Reducer::initialize_local_used_map() {
   at::TensorOptions options;
   options = options.dtype(at::kInt);
 
+  //TODO(zhiguangda): remove this when CNCL can garantee the performance of Int type
+  if (params_[0].is_mlu()) {
+    options = options.dtype(at::kFloat);
+  }
+
   // Deliberately don't pin the memory even if local_used_map_dev_ will
   // be cuda. See Note [local_used_map_ -> local_used_map_dev copying]
   local_used_map_ = at::zeros({static_cast<long>(variable_count)}, options);
@@ -685,7 +690,8 @@ void Reducer::autograd_hook(size_t index) {
 void Reducer::all_reduce_local_used_map() {
   // See Note [Skip allreducing local_used_map_dev]
   // H2D from local_used_map_ to local_used_map_dev_
-  if (local_used_map_dev_.is_cuda()) {
+  if (local_used_map_dev_.is_cuda() ||
+      local_used_map_dev_.is_mlu()) {
     // Note [local_used_map_ -> local_used_map_dev copying]
     // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     // We do async H2D to avoid the blocking overhead. The async copy and
