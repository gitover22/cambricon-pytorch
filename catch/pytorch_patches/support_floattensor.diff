diff --git a/aten/src/ATen/core/TensorBase.h b/aten/src/ATen/core/TensorBase.h
index 8ec5670664a..b86589e9d08 100644
--- a/aten/src/ATen/core/TensorBase.h
+++ b/aten/src/ATen/core/TensorBase.h
@@ -405,6 +405,12 @@ class TORCH_API TensorBase {
     // NB: this is not a native function to avoid dispatching overhead.
     return impl_->is_cuda();
   }
+  
+  /// Returns if a `Tensor` has CUDA backend.
+  bool is_mlu() const {
+    // NB: this is not a native function to avoid dispatching overhead.
+    return impl_->is_mlu();
+  }
 
   /// Returns if a `Tensor` has IPU backend.
   bool is_ipu() const {
diff --git a/c10/core/TensorImpl.h b/c10/core/TensorImpl.h
index f110b0e9fa4..13ff38b0420 100644
--- a/c10/core/TensorImpl.h
+++ b/c10/core/TensorImpl.h
@@ -1114,6 +1114,13 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
     }
     return device_opt_.has_value() && device_opt_->type() == kXLA;
   }
+  
+  bool is_mlu() const {
+    if (C10_UNLIKELY(device_policy_)) {
+      return device_custom().is_mlu();
+    }
+    return device_opt_.has_value() && device_opt_->type() == kMLU;
+  }
 
   bool is_hpu() const {
     if (C10_UNLIKELY(device_policy_)) {
diff --git a/tools/autograd/templates/VariableType.h b/tools/autograd/templates/VariableType.h
index ad2abc2bdb7..c72f3695ecf 100644
--- a/tools/autograd/templates/VariableType.h
+++ b/tools/autograd/templates/VariableType.h
@@ -46,6 +46,7 @@ using c10::optional;
 
 namespace VariableType {
   TORCH_API std::vector<at::DeprecatedTypeProperties*> allCUDATypes();
+  TORCH_API std::vector<at::DeprecatedTypeProperties*> allMLUTypes();
   TORCH_API std::vector<at::DeprecatedTypeProperties*> allCPUTypes();
 
   at::Tensor & unpack(Tensor & t, const char * name, int pos);
diff --git a/torch/csrc/autograd/VariableTypeManual.cpp b/torch/csrc/autograd/VariableTypeManual.cpp
index e276521aceb..f6d9cc671b7 100644
--- a/torch/csrc/autograd/VariableTypeManual.cpp
+++ b/torch/csrc/autograd/VariableTypeManual.cpp
@@ -39,6 +39,10 @@ C10_EXPORT std::vector<at::DeprecatedTypeProperties*> allCPUTypes() {
   return allTypesForBackends({Backend::CPU, Backend::SparseCPU});
 }
 
+C10_EXPORT std::vector<at::DeprecatedTypeProperties*> allMLUTypes() {
+  return allTypesForBackends({ Backend::MLU });
+}
+
 C10_EXPORT std::vector<at::DeprecatedTypeProperties*> allCUDATypes() {
   at::globalContext().lazyInitCUDA();
   return allTypesForBackends({Backend::CUDA, Backend::SparseCUDA});
diff --git a/torch/csrc/autograd/python_variable.cpp b/torch/csrc/autograd/python_variable.cpp
index 342e200089c..ba343b65561 100644
--- a/torch/csrc/autograd/python_variable.cpp
+++ b/torch/csrc/autograd/python_variable.cpp
@@ -1311,6 +1311,16 @@ PyObject* THPVariable_is_cuda(THPVariable* self, void* unused) {
   END_HANDLE_TH_ERRORS
 }
 
+PyObject* THPVariable_is_mlu(THPVariable* self, void* unused) {
+  HANDLE_TH_ERRORS
+  if (check_has_torch_function((PyObject*)self)) {
+    return handle_torch_function_getter(self, "is_mlu");
+  }
+  auto& self_ = THPVariable_Unpack(self);
+  return torch::autograd::utils::wrap(self_.is_mlu());
+  END_HANDLE_TH_ERRORS
+}
+
 PyObject* THPVariable_is_ipu(THPVariable* self, void* unused) {
   HANDLE_TH_ERRORS
   if (check_has_torch_function((PyObject*)self)) {
@@ -1558,6 +1568,7 @@ static struct PyGetSetDef THPVariable_properties[] = {
     {"name", (getter)THPVariable_get_name, nullptr, nullptr, nullptr},
     {"shape", (getter)THPVariable_get_shape, nullptr, nullptr, nullptr},
     {"is_cuda", (getter)THPVariable_is_cuda, nullptr, nullptr, nullptr},
+    {"is_mlu", (getter)THPVariable_is_cuda, nullptr, nullptr, nullptr},
     {"is_cpu", (getter)THPVariable_is_cpu, nullptr, nullptr, nullptr},
     {"is_xpu", (getter)THPVariable_is_xpu, nullptr, nullptr, nullptr},
     {"is_ipu", (getter)THPVariable_is_ipu, nullptr, nullptr, nullptr},
diff --git a/torch/csrc/tensor/python_tensor.cpp b/torch/csrc/tensor/python_tensor.cpp
index 648fd041868..0f3c6fe7830 100644
--- a/torch/csrc/tensor/python_tensor.cpp
+++ b/torch/csrc/tensor/python_tensor.cpp
@@ -36,6 +36,7 @@ struct PyTensorType {
   THPDtype* dtype;
   THPLayout* layout;
   bool is_cuda;
+  bool is_mlu;
   // NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,cppcoreguidelines-avoid-magic-numbers,modernize-avoid-c-arrays)
   char name[64];
   int backend;
@@ -129,6 +130,14 @@ PyObject* Tensor_is_cuda(PyTensorType* self, void* unused) {
   }
 }
 
+PyObject *Tensor_is_mlu(PyTensorType* self, void *unused) {
+  if (self->is_mlu) {
+    Py_RETURN_TRUE;
+  } else {
+    Py_RETURN_FALSE;
+  }
+}
+
 PyObject* Tensor_is_sparse(PyTensorType* self, void* unused) {
   if (self->layout->layout == at::Layout::Strided) {
     Py_RETURN_FALSE;
@@ -157,6 +166,7 @@ static struct PyGetSetDef metaclass_properties[] = {
     {"dtype", (getter)Tensor_dtype, nullptr, nullptr, nullptr},
     {"layout", (getter)Tensor_layout, nullptr, nullptr, nullptr},
     {"is_cuda", (getter)Tensor_is_cuda, nullptr, nullptr, nullptr},
+    {"is_mlu",  (getter)Tensor_is_mlu, nullptr, nullptr, nullptr},
     {"is_sparse", (getter)Tensor_is_sparse, nullptr, nullptr, nullptr},
     {"is_sparse_csr", (getter)Tensor_is_sparse_csr, nullptr, nullptr, nullptr},
     {nullptr}};
@@ -209,6 +219,8 @@ static const char* get_module(Backend backend) {
       return "torch";
     case Backend::CUDA:
       return "torch.cuda";
+    case Backend::MLU:
+      return "torch.mlu";
     case Backend::SparseCPU:
       return "torch.sparse";
     case Backend::SparseCUDA:
@@ -250,6 +262,7 @@ static void set_type(
   type_obj.dtype = torch::getTHPDtype(scalarType);
   type_obj.is_cuda =
       (backend == at::Backend::CUDA || backend == at::Backend::SparseCUDA);
+  type_obj.is_mlu = (backend == at::Backend::MLU);
 }
 
 static void set_name(PyTensorType& type_obj, const std::string& name) {
diff --git a/torch/csrc/utils/python_arg_parser.h b/torch/csrc/utils/python_arg_parser.h
index 9b23af58297..aabf1294d11 100644
--- a/torch/csrc/utils/python_arg_parser.h
+++ b/torch/csrc/utils/python_arg_parser.h
@@ -340,8 +340,10 @@ struct PythonArgs {
   inline at::Layout layoutWithDefault(int i, at::Layout default_layout);
   inline c10::optional<at::Layout> layoutOptional(int i);
   inline at::Device device(int i);
+  inline at::Device device(int i, c10::DispatchKey dispatch_key);
   inline at::Device deviceWithDefault(int i, const at::Device& default_device);
   inline c10::optional<at::Device> deviceOptional(int i);
+  inline c10::optional<at::Device> deviceOptional(int i, c10::DispatchKey dispatch_key);
   inline at::Dimname dimname(int i);
   inline std::vector<at::Dimname> dimnamelist(int i);
   inline c10::optional<std::vector<at::Dimname>> toDimnameListOptional(int i);
@@ -846,6 +848,24 @@ inline at::Device toDevice(PyObject* obj) {
   return at::Device(device_str);
 }
 
+inline at::Device toDevice(PyObject* obj, c10::DispatchKey dispatch_key) {
+  if (THPDevice_Check(obj)) {
+    const auto device = reinterpret_cast<THPDevice*>(obj);
+    return device->device;
+  }
+  if (THPUtils_checkLong(obj)) {
+    const auto device_index = THPUtils_unpackLong(obj);
+    TORCH_CHECK(device_index >= 0, "Device index must not be negative");
+    if (dispatch_key != c10::DispatchKey::MLU) {
+        return at::Device(DeviceType::CUDA, device_index);
+    } else {
+        return at::Device(DeviceType::MLU, device_index);
+    }
+  }
+  const std::string& device_str = THPUtils_unpackString(obj);
+  return at::Device(device_str);
+}
+
 inline at::Device PythonArgs::device(int i) {
   if (!args[i]) {
     return torch::tensors::get_default_device();
@@ -853,6 +873,13 @@ inline at::Device PythonArgs::device(int i) {
   return toDevice(args[i]);
 }
 
+inline at::Device PythonArgs::device(int i, c10::DispatchKey dispatch_key) {
+  if (!args[i]) {
+    return torch::tensors::get_default_device();
+  }
+  return toDevice(args[i], dispatch_key);
+}
+
 inline at::Device PythonArgs::deviceWithDefault(
     int i,
     const at::Device& default_device) {
@@ -867,6 +894,12 @@ inline c10::optional<at::Device> PythonArgs::deviceOptional(int i) {
   return device(i);
 }
 
+inline c10::optional<at::Device> PythonArgs::deviceOptional(int i, c10::DispatchKey dispatch_key) {
+  if (!args[i])
+    return c10::nullopt;
+  return device(i, dispatch_key);
+}
+
 inline at::Dimname PythonArgs::dimname(int i) {
   TORCH_INTERNAL_ASSERT(args[i] != nullptr);
   return THPDimname_parse(args[i]);
diff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index 707ebeb19e8..97a4966729c 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -630,7 +630,7 @@ Tensor legacy_tensor_generic_ctor_new(
   ParsedArgs<2> parsed_args;
   auto r = parser.parse(args, kwargs, parsed_args);
   if (r.idx == 0) {
-    auto deviceOptional = r.deviceOptional(0);
+    auto deviceOptional = r.deviceOptional(0, dispatch_key);
     check_legacy_ctor_device(dispatch_key, deviceOptional);
     at::OptionalDeviceGuard device_guard(deviceOptional);
     return at::empty({0}, build_options(options, scalar_type));
@@ -680,7 +680,7 @@ Tensor legacy_tensor_generic_ctor_new(
     }
   } else if (r.idx == 5) {
     PyObject* arg = r.pyobject(0);
-    auto deviceOptional = r.deviceOptional(1);
+    auto deviceOptional = r.deviceOptional(1, dispatch_key);
     check_legacy_ctor_device(dispatch_key, deviceOptional);
     if (!THPSize_Check(arg) && PyTuple_GET_SIZE(args) >= 1 &&
         arg == PyTuple_GET_ITEM(args, 0)) {
@@ -690,9 +690,9 @@ Tensor legacy_tensor_generic_ctor_new(
           options, scalar_type, deviceOptional, r.pyobject(0));
     }
     return new_with_sizes(
-        options, scalar_type, r.deviceOptional(1), r.intlist(0));
+        options, scalar_type, r.deviceOptional(1, dispatch_key), r.intlist(0));
   } else if (r.idx == 6) {
-    auto deviceOptional = r.deviceOptional(1);
+    auto deviceOptional = r.deviceOptional(1, dispatch_key);
     check_legacy_ctor_device(dispatch_key, deviceOptional);
     return legacy_new_from_sequence(
         options, scalar_type, deviceOptional, r.pyobject(0));
diff --git a/torch/csrc/utils/tensor_types.cpp b/torch/csrc/utils/tensor_types.cpp
index 8a20c93d87a..b08ede1e66d 100644
--- a/torch/csrc/utils/tensor_types.cpp
+++ b/torch/csrc/utils/tensor_types.cpp
@@ -66,11 +66,14 @@ std::string type_to_string(const at::DeprecatedTypeProperties& type) {
 
 at::TensorOptions options_from_string(const std::string& str) {
   static std::string cuda_prefix("torch.cuda.");
+  static std::string mlu_prefix("torch.mlu.");
   static c10::once_flag cpu_once;
   static c10::once_flag cuda_once;
+  static c10::once_flag mlu_once;
   static std::unordered_map<std::string, at::DeprecatedTypeProperties*> cpu_map;
   static std::unordered_map<std::string, at::DeprecatedTypeProperties*>
       cuda_map;
+  static std::unordered_map<std::string, at::DeprecatedTypeProperties*> mlu_map;
 
   const std::unordered_map<std::string, at::DeprecatedTypeProperties*>* map =
       nullptr;
@@ -91,6 +94,16 @@ at::TensorOptions options_from_string(const std::string& str) {
       }
     });
     map = &cuda_map;
+  } else if (
+      std::mismatch(mlu_prefix.begin(), mlu_prefix.end(), str.begin()).first ==
+      mlu_prefix.end()) {
+    // torch.mlu. is prefix of str
+    c10::call_once(mlu_once, []() {
+      for (auto type : autograd::VariableType::allMLUTypes()) {
+        mlu_map.emplace(type_to_string(*type), type);
+      }
+    });
+    map = &mlu_map;
   } else {
     c10::call_once(cpu_once, []() {
       for (auto type : autograd::VariableType::allCPUTypes()) {
@@ -114,7 +127,7 @@ std::vector<std::pair<Backend, ScalarType>> all_declared_types() {
   // of legacy tensor types e.g. torch.cuda.FloatTensor which are
   // maintained for backwards-compatibility only.
   std::vector<Backend> backends = {
-      Backend::CPU, Backend::CUDA, Backend::SparseCPU, Backend::SparseCUDA};
+      Backend::MLU, Backend::CPU, Backend::CUDA, Backend::SparseCPU, Backend::SparseCUDA};
   std::vector<ScalarType> scalar_types = {
       ScalarType::Byte,
       ScalarType::Char,
diff --git a/torch/mlu/__init__.py b/torch/mlu/__init__.py
new file mode 100644
index 00000000000..e69de29bb2d
