diff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp
index 8cd5e0c785..a61047cf65 100644
--- a/torch/csrc/autograd/engine.cpp
+++ b/torch/csrc/autograd/engine.cpp
@@ -608,6 +608,7 @@ void GraphTask::exec_post_processing() {
   // Syncs caller_current_stream with leaf streams, so final_callbacks may use
   // any grad on its device's current stream.
   if (leaf_streams.size() > 0) {
+    const auto dev_type = c10::impl::hasDeviceGuardImpl(c10::DeviceType::MLU) ? c10::DeviceType::MLU : c10::DeviceType::CUDA;
     for (const auto& leaf_stream : leaf_streams) {
       // stash_current_streams() stashed streams for all device IDs that already
       // had a CUDA context before the GraphTask executed. For inactive devices,
@@ -619,7 +620,7 @@ void GraphTask::exec_post_processing() {
           *caller_current_streams_[leaf_stream.device_index()];

       if (caller_current_stream != leaf_stream) {
-        auto event = c10::Event{c10::DeviceType::CUDA};
+        auto event = c10::Event{dev_type};
         event.record(leaf_stream);
         caller_current_stream.wait(event);
       }
@@ -866,7 +867,10 @@ void Engine::evaluate_function(
   // ensure they're safe to consume in the context of the present
   // func's stream (if applicable). So we guard onto that stream
   // before working with the grads in any capacity.
-  const auto opt_parent_stream = (*func).stream(c10::DeviceType::CUDA);
+  auto opt_parent_stream = (*func).stream(c10::DeviceType::MLU);
+  if (!opt_parent_stream.has_value()) {
+    opt_parent_stream = (*func).stream(c10::DeviceType::CUDA);
+  }
   c10::OptionalStreamGuard parent_stream_guard{opt_parent_stream};

   // If exec_info_ is not empty, we have to instrument the execution
@@ -963,7 +967,10 @@ void Engine::evaluate_function(
       InputBuffer input_buffer(next.function->num_inputs());

       // Accumulates into buffer
-      const auto opt_next_stream = next.function->stream(c10::DeviceType::CUDA);
+      auto opt_next_stream = next.function->stream(c10::DeviceType::MLU);
+      if (!opt_next_stream.has_value()) {
+        opt_next_stream = next.function->stream(c10::DeviceType::CUDA);
+      }
       input_buffer.add(
           next.input_nr, std::move(output), opt_parent_stream, opt_next_stream);

@@ -979,7 +986,10 @@ void Engine::evaluate_function(
       auto& input_buffer = not_ready_it->second;

       // Accumulates into buffer
-      const auto opt_next_stream = next.function->stream(c10::DeviceType::CUDA);
+      auto opt_next_stream = next.function->stream(c10::DeviceType::MLU);
+      if (!opt_next_stream.has_value()) {
+        opt_next_stream = next.function->stream(c10::DeviceType::CUDA);
+      }
       input_buffer.add(
           next.input_nr, std::move(output), opt_parent_stream, opt_next_stream);
       if (is_ready) {
@@ -1013,6 +1023,7 @@ auto Engine::compute_dependencies(
   std::vector<Node*> queue{root};
   bool might_use_cuda = at::globalContext().hasCUDA();
   bool will_use_cuda = false;
+  bool will_use_mlu = false;

   // Queue contains all nodes that will start propagating gradients.
   // We no longer have to expand functions that don't require grad.
@@ -1025,6 +1036,8 @@ auto Engine::compute_dependencies(
     }
     if (might_use_cuda && !will_use_cuda) {
       will_use_cuda = fn->stream(c10::DeviceType::CUDA).has_value();
+    } else if (!will_use_mlu) {
+      will_use_mlu = fn->stream(c10::DeviceType::MLU).has_value();
     }
     for (const auto& edge : fn->next_edges()) {
       if (auto next_ptr = edge.function.get()) {
@@ -1036,7 +1049,7 @@ auto Engine::compute_dependencies(
     }
   }

-  if (will_use_cuda) {
+  if (will_use_cuda || will_use_mlu) {
     // Collects current streams for devices where this process has a context,
     // so GraphTask::exec_post_processing can sync them with leaf_streams.
     task.stash_current_streams();
@@ -1104,8 +1117,12 @@ auto Engine::execute(
     auto input = inputs.at(0);

     const auto input_stream = InputMetadata(input).stream();
-    const auto opt_next_stream =
-        roots.at(0).function->stream(c10::DeviceType::CUDA);
+    auto opt_next_stream =
+        roots.at(0).function->stream(c10::DeviceType::MLU);
+    if (!opt_next_stream.has_value()) {
+      opt_next_stream =
+          roots.at(0).function->stream(c10::DeviceType::CUDA);
+    }
     input_buffer.add(
         roots.at(0).input_nr, std::move(input), input_stream, opt_next_stream);

@@ -1360,7 +1377,8 @@ void Engine::add_thread_pool_task(const std::weak_ptr<GraphTask>& graph_task) {
 // Only called if Engine::execute detects at least one node runs on a cuda
 // stream.
 void GraphTask::stash_current_streams() {
-  const auto guard = c10::impl::VirtualGuardImpl{c10::DeviceType::CUDA};
+  const auto dev_type = c10::impl::hasDeviceGuardImpl(c10::DeviceType::MLU) ? c10::DeviceType::MLU : c10::DeviceType::CUDA;
+  const auto guard = c10::impl::VirtualGuardImpl{dev_type};
   auto num_gpus = guard.deviceCount();
   caller_current_streams_.resize(num_gpus);
   if (num_gpus > 0) {
@@ -1376,7 +1394,7 @@ void GraphTask::stash_current_streams() {
       if (at::detail::getCUDAHooks().hasPrimaryContext(idx)) {
 #endif
         caller_current_streams_[idx] =
-            guard.getStream({c10::DeviceType::CUDA, idx});
+            guard.getStream({dev_type, idx});
       } else {
         caller_current_streams_[idx] = c10::nullopt;
       }


diff --git a/torch/csrc/autograd/input_buffer.cpp b/torch/csrc/autograd/input_buffer.cpp
index 6cc6acefc9..cd43bf3a3c 100644
--- a/torch/csrc/autograd/input_buffer.cpp
+++ b/torch/csrc/autograd/input_buffer.cpp
@@ -25,7 +25,7 @@ namespace {
 // TODO: clean this up when https://github.com/pytorch/pytorch/issues/60306 is
 // improved
 void record_stream_any_impl(Variable& var, c10::Stream& stream) {
-  const auto guard = c10::impl::VirtualGuardImpl(c10::DeviceType::CUDA);
+  const auto guard = c10::impl::VirtualGuardImpl(device_of(var).value().type());

   if (C10_UNLIKELY(at::isBatchedTensor(var))) {
     auto* impl = at::maybeGetBatchedImpl(var);
@@ -130,7 +130,7 @@ void InputBuffer::add(

   TORCH_INTERNAL_ASSERT(device_of(var));
   c10::optional<c10::Stream> opt_accumulate_stream = c10::nullopt;
-  if (device_of(var)->is_cuda()) {
+  if (device_of(var)->is_cuda() || device_of(var)->is_mlu()) {
     const auto on_producer =
         opt_producer_stream && device_of(var) == opt_producer_stream->device();
     const auto on_consumer =
@@ -141,14 +141,14 @@ void InputBuffer::add(
       opt_accumulate_stream = opt_consumer_stream;
       if (opt_accumulate_stream != opt_producer_stream) {
         // (2b)
-        auto event = c10::Event{c10::DeviceType::CUDA};
+        auto event = c10::Event{device_of(var).value().type()};
         event.record(*opt_producer_stream);
         opt_accumulate_stream->wait(event);
         record_stream_any_impl(var, *opt_accumulate_stream);
       }
     } else {
       c10::optional<c10::Stream> opt_sync_stream = c10::nullopt;
-      const auto guard = c10::impl::VirtualGuardImpl{c10::DeviceType::CUDA};
+      const auto guard = c10::impl::VirtualGuardImpl{device_of(var).value().type()};
       if (on_consumer && !on_producer) {
         // (3a)
         opt_accumulate_stream = opt_consumer_stream;
@@ -165,10 +165,10 @@ void InputBuffer::add(
       if (opt_sync_stream && (opt_accumulate_stream != opt_sync_stream)) {
         // (3b), (4b)
         c10::OptionalDeviceGuard device_guard{opt_sync_stream->device()};
-        auto event = c10::Event{c10::DeviceType::CUDA};
+        auto event = c10::Event{device_of(var).value().type()};
         event.record(*opt_sync_stream);
         opt_accumulate_stream->wait(event);
-        const auto guard = c10::impl::VirtualGuardImpl(c10::DeviceType::CUDA);
+        const auto guard = c10::impl::VirtualGuardImpl(device_of(var).value().type());
         record_stream_any_impl(var, *opt_accumulate_stream);
       }
     }
